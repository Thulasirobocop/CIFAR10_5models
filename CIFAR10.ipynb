{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5dlD7Mq1nYB"
      },
      "source": [
        "!pip install -q keras\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add,Activation,BatchNormalization, Convolution2D, MaxPooling2D,BatchNormalization\n",
        "from keras.utils import np_utils \n",
        "from keras.datasets import mnist\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.datasets import cifar10\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k55fr1ot3G4w"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKdGeBKv3ffZ",
        "outputId": "3a219bfb-69ba-4f13-c44b-9d53421a691d"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "(50000, 1)\n",
            "(10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "GsPB7g653p4Q",
        "outputId": "aa548ab8-8484-4985-b079-2dd4a4ea7a14"
      },
      "source": [
        "plt.imshow(X_train[1])\n",
        "print(y_train[1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKI_zRZm5FaJ"
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 32, 32,3)\n",
        "X_test = X_test.reshape(X_test.shape[0], 32,32,3)\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYh80dX153EU",
        "outputId": "88a2be59-e0a1-4ccc-d251-83cd727c1a8b"
      },
      "source": [
        "model1 = Sequential()\n",
        "\n",
        "model1.add(Convolution2D(10, 3, activation='relu', input_shape=(32,32,3))) # channel dimensions = 30x30x10    \n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.2))\n",
        "\n",
        "model1.add(Convolution2D(16, 3, activation='relu'))                        # channel dimensions = 28x28x16    \n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.2))\n",
        "\n",
        "model1.add(Convolution2D(16, 1, activation='relu'))                        # channel dimensions = 28x28x16    \n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))                                 # channel dimensions = 14x14x16   \n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model1.add(Convolution2D(32, 3, activation='relu'))#10                     # channel dimensions = 12x12x32    \n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.2))\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model1.add(Convolution2D(32, 3, activation='relu'))#8                      # channel dimensions = 10x10x32    \n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.2))\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model1.add(Convolution2D(32, 3, activation='relu'))#6                      # channel dimensions = 8x8x32   \n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.2))\n",
        "\n",
        "model1.add(Convolution2D(16, 3, activation='relu'))#6                      # channel dimensions = 6x6x16   \n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.2))\n",
        "\n",
        "model1.add(Convolution2D(16, 3, activation='relu'))#6                      # channel dimensions = 4x4x16   \n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.2))\n",
        "\n",
        "# Performing only 2dconvolution at the last convolution layer(no batchnormalization and dropout)\n",
        "model1.add(Convolution2D(10, 4))                                           # using 4x4 kernel to see the complete image\n",
        "\n",
        "# Here we are Flateening our dat i.e making it one dimensional which we will feed to the network.\n",
        "model1.add(Flatten())\n",
        "#Using softmax activation function at the last layer which is used for multi class classification\n",
        "model1.add(Activation('softmax'))\n",
        "model1.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 30, 30, 10)        280       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 30, 30, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 30, 30, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 28, 28, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 28, 28, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 16)        272       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 12, 12, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 12, 12, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 10, 10, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 10, 10, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 10, 10, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 32)          9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 8, 8, 32)          128       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 6, 6, 16)          4624      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 6, 6, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 4, 4, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 4, 4, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 1, 1, 10)          2570      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 35,274\n",
            "Trainable params: 34,966\n",
            "Non-trainable params: 308\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUKE0ms-78QC",
        "outputId": "0cc3b3c8-2aa7-4130-fa04-c8d64116d672"
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "#\tLearningRate = LearningRate * 1/(1 + decay * epoch) here decay is 0.319 and epoch is 10.\n",
        "\n",
        "# here we are compiling our model and using 'categorical_crossentropy' as our loss function and adam as our optimizer with learning rate =0.003 and metrics is accuracy\n",
        "model1.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "# Here we are traing our model using the data and using batch size of 128,number of epochs are 20 and using verbose=1 for printing out all the results.\n",
        "# In the callbacks parameter we are using the LearningRateScheduler which takes two arguments scheduler function which we built earlier to reduce the learning rate in each decay and verbose =1\n",
        "model1.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.003.\n",
            "391/391 [==============================] - 52s 13ms/step - loss: 1.9996 - accuracy: 0.2878 - val_loss: 2.2838 - val_accuracy: 0.2575\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0022744503.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 1.3875 - accuracy: 0.4934 - val_loss: 1.4132 - val_accuracy: 0.4942\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0018315018.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 1.2474 - accuracy: 0.5516 - val_loss: 1.2246 - val_accuracy: 0.5661\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0015329586.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 1.1619 - accuracy: 0.5796 - val_loss: 1.1847 - val_accuracy: 0.5818\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0013181019.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 1.0970 - accuracy: 0.6068 - val_loss: 1.2693 - val_accuracy: 0.5538\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0011560694.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 1.0618 - accuracy: 0.6220 - val_loss: 1.0519 - val_accuracy: 0.6232\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0010295127.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 1.0193 - accuracy: 0.6343 - val_loss: 1.0655 - val_accuracy: 0.6259\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0009279307.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.9952 - accuracy: 0.6433 - val_loss: 1.0411 - val_accuracy: 0.6390\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0008445946.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.9788 - accuracy: 0.6506 - val_loss: 1.0150 - val_accuracy: 0.6383\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0007749935.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.9535 - accuracy: 0.6593 - val_loss: 1.0486 - val_accuracy: 0.6354\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0007159905.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.9372 - accuracy: 0.6654 - val_loss: 0.9746 - val_accuracy: 0.6561\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.000665336.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.9155 - accuracy: 0.6740 - val_loss: 0.9659 - val_accuracy: 0.6613\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0006213753.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.9045 - accuracy: 0.6806 - val_loss: 0.8930 - val_accuracy: 0.6853\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0005828638.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.8904 - accuracy: 0.6827 - val_loss: 0.8642 - val_accuracy: 0.6916\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0005488474.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.8726 - accuracy: 0.6899 - val_loss: 0.8818 - val_accuracy: 0.6842\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0005185825.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.8679 - accuracy: 0.6926 - val_loss: 0.8613 - val_accuracy: 0.6928\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.000491481.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.8743 - accuracy: 0.6913 - val_loss: 0.8641 - val_accuracy: 0.6940\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0004670715.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.8531 - accuracy: 0.6968 - val_loss: 0.8455 - val_accuracy: 0.6973\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0004449718.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.8495 - accuracy: 0.6988 - val_loss: 0.8490 - val_accuracy: 0.6990\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.000424869.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.8361 - accuracy: 0.7039 - val_loss: 0.8860 - val_accuracy: 0.6876\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f550049d0d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFyq7Sjv-VT9",
        "outputId": "3307fe2c-cb5a-426c-95bc-1249d34f4c8a"
      },
      "source": [
        "score = model1.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "y_pred = model1.predict(X_test)\n",
        "index1={}\n",
        "for i, k in enumerate(y_pred[:1][0]):\n",
        "  index1.update({k:i})\n",
        "print('Prediction:',index1[max(index1)])\n",
        "print('Actual',y_test[:1][0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.8860236406326294, 0.6876000165939331]\n",
            "Prediction: 3\n",
            "Actual [3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH7DfYci8wld",
        "outputId": "d860d0f5-38cf-457f-91f9-997c72917cb7"
      },
      "source": [
        "model2 = Sequential()\n",
        "\n",
        "model2.add(Convolution2D(10, 3, activation='relu', input_shape=(32,32,3))) # channel dimensions = 30x30x10    \n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.2))\n",
        "\n",
        "model2.add(Convolution2D(16, 3, activation='relu'))                        # channel dimensions = 28x28x16    \n",
        "model2.add(BatchNormalization())\n",
        "#model2.add(Dropout(0.2))\n",
        "\n",
        "model2.add(Convolution2D(16, 1, activation='relu'))                        # channel dimensions = 28x28x16    \n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))                                 # channel dimensions = 14x14x16   \n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model2.add(Convolution2D(32, 3, activation='relu'))#10                     # channel dimensions = 12x12x32    \n",
        "model2.add(BatchNormalization())\n",
        "#model2.add(Dropout(0.2))\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model2.add(Convolution2D(64, 3, activation='relu'))#8                      # channel dimensions = 10x10x64    \n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.2))\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model2.add(Convolution2D(64, 3, activation='relu'))#6                      # channel dimensions = 8x8x64   \n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.2))\n",
        "\n",
        "model2.add(Convolution2D(32, 3, activation='relu'))#6                      # channel dimensions = 6x6x32   \n",
        "model2.add(BatchNormalization())\n",
        "#model2.add(Dropout(0.2))\n",
        "\n",
        "model2.add(Convolution2D(16, 3, activation='relu'))#6                      # channel dimensions = 4x4x16   \n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.2))\n",
        "\n",
        "# Performing only 2dconvolution at the last convolution layer(no batchnormalization and dropout)\n",
        "model2.add(Convolution2D(10, 4))                                           # using 4x4 kernel to see the complete image\n",
        "\n",
        "# Here we are Flateening our dat i.e making it one dimensional which we will feed to the network.\n",
        "model2.add(Flatten())\n",
        "#Using softmax activation function at the last layer which is used for multi class classification\n",
        "model2.add(Activation('softmax'))\n",
        "model2.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 30, 30, 10)        280       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 30, 30, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 30, 30, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 28, 28, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 28, 28, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 28, 28, 16)        272       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 12, 12, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 12, 12, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 10, 10, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 10, 10, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 10, 10, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 8, 8, 64)          36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 6, 6, 32)          18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 6, 6, 32)          128       \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 4, 4, 16)          4624      \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 4, 4, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 1, 1, 10)          2570      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 88,666\n",
            "Trainable params: 88,198\n",
            "Non-trainable params: 468\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQp2IsmP_Swh",
        "outputId": "4bc2ed7d-c478-4a44-9be6-5452e197c00f"
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "  return round(0.002 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "#\tLearningRate = LearningRate * 1/(1 + decay * epoch) here decay is 0.319 and epoch is 10.\n",
        "\n",
        "# here we are compiling our model and using 'categorical_crossentropy' as our loss function and adam as our optimizer with learning rate =0.003 and metrics is accuracy\n",
        "model2.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.002), metrics=['accuracy'])\n",
        "\n",
        "# Here we are traing our model using the data and using batch size of 128,number of epochs are 20 and using verbose=1 for printing out all the results.\n",
        "# In the callbacks parameter we are using the LearningRateScheduler which takes two arguments scheduler function which we built earlier to reduce the learning rate in each decay and verbose =1\n",
        "model2.fit(X_train, Y_train, batch_size=128, epochs=25, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.002.\n",
            "391/391 [==============================] - 7s 13ms/step - loss: 1.2287 - accuracy: 0.5603 - val_loss: 1.2252 - val_accuracy: 0.5859\n",
            "Epoch 2/25\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0015163002.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 1.0136 - accuracy: 0.6416 - val_loss: 1.2917 - val_accuracy: 0.5641\n",
            "Epoch 3/25\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0012210012.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.9049 - accuracy: 0.6797 - val_loss: 0.8776 - val_accuracy: 0.6899\n",
            "Epoch 4/25\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0010219724.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.8270 - accuracy: 0.7085 - val_loss: 0.8527 - val_accuracy: 0.7069\n",
            "Epoch 5/25\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0008787346.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.7666 - accuracy: 0.7261 - val_loss: 0.8564 - val_accuracy: 0.7029\n",
            "Epoch 6/25\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0007707129.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.7217 - accuracy: 0.7465 - val_loss: 0.7954 - val_accuracy: 0.7267\n",
            "Epoch 7/25\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0006863418.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.6959 - accuracy: 0.7539 - val_loss: 0.7558 - val_accuracy: 0.7336\n",
            "Epoch 8/25\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0006186205.\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.6674 - accuracy: 0.7667 - val_loss: 0.7535 - val_accuracy: 0.7408\n",
            "Epoch 9/25\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0005630631.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.6421 - accuracy: 0.7757 - val_loss: 0.8050 - val_accuracy: 0.7337\n",
            "Epoch 10/25\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0005166624.\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.6351 - accuracy: 0.7739 - val_loss: 0.6991 - val_accuracy: 0.7658\n",
            "Epoch 11/25\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.000477327.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.6040 - accuracy: 0.7883 - val_loss: 0.7016 - val_accuracy: 0.7603\n",
            "Epoch 12/25\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0004435573.\n",
            "391/391 [==============================] - 4s 12ms/step - loss: 0.5935 - accuracy: 0.7926 - val_loss: 0.6892 - val_accuracy: 0.7666\n",
            "Epoch 13/25\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0004142502.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.5818 - accuracy: 0.7972 - val_loss: 0.6819 - val_accuracy: 0.7704\n",
            "Epoch 14/25\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0003885759.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.5685 - accuracy: 0.8005 - val_loss: 0.7156 - val_accuracy: 0.7624\n",
            "Epoch 15/25\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0003658983.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.5653 - accuracy: 0.7991 - val_loss: 0.6917 - val_accuracy: 0.7682\n",
            "Epoch 16/25\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0003457217.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.5479 - accuracy: 0.8062 - val_loss: 0.6629 - val_accuracy: 0.7771\n",
            "Epoch 17/25\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.000327654.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.5398 - accuracy: 0.8106 - val_loss: 0.6610 - val_accuracy: 0.7795\n",
            "Epoch 18/25\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.000311381.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.5320 - accuracy: 0.8130 - val_loss: 0.6675 - val_accuracy: 0.7757\n",
            "Epoch 19/25\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0002966479.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.5252 - accuracy: 0.8148 - val_loss: 0.6681 - val_accuracy: 0.7765\n",
            "Epoch 20/25\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.000283246.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.5250 - accuracy: 0.8165 - val_loss: 0.6464 - val_accuracy: 0.7852\n",
            "Epoch 21/25\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0002710027.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.5156 - accuracy: 0.8210 - val_loss: 0.6662 - val_accuracy: 0.7768\n",
            "Epoch 22/25\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.000259774.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.5033 - accuracy: 0.8224 - val_loss: 0.6973 - val_accuracy: 0.7752\n",
            "Epoch 23/25\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0002494388.\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 0.5054 - accuracy: 0.8203 - val_loss: 0.6790 - val_accuracy: 0.7756\n",
            "Epoch 24/25\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0002398944.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.5045 - accuracy: 0.8231 - val_loss: 0.6641 - val_accuracy: 0.7809\n",
            "Epoch 25/25\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0002310536.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.4967 - accuracy: 0.8248 - val_loss: 0.6517 - val_accuracy: 0.7839\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f54ad1299d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FR83kHD1-euk",
        "outputId": "ffc804fe-c971-407d-a8e9-a66c8e9e6dca"
      },
      "source": [
        "score = model2.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "y_pred = model2.predict(X_test)\n",
        "index1={}\n",
        "for i, k in enumerate(y_pred[2:3][0]):\n",
        "  index1.update({k:i})\n",
        "print('Prediction:',index1[max(index1)])\n",
        "print('Actual',y_test[2:3][0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6516966819763184, 0.7839000225067139]\n",
            "Prediction: 8\n",
            "Actual [8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n1iLy9q_ewc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1d695e-7249-4026-8c61-4c1bb9c9e599"
      },
      "source": [
        "model3 = Sequential()\n",
        "\n",
        "model3.add(Convolution2D(10, 3, activation='relu', input_shape=(32,32,3))) # channel dimensions = 30x30x10    \n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.3))\n",
        "\n",
        "model3.add(Convolution2D(16, 3, activation='relu'))                        # channel dimensions = 28x28x16    \n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.2))\n",
        "\n",
        "model3.add(Convolution2D(16, 1, activation='relu'))                        # channel dimensions = 28x28x16    \n",
        "model3.add(MaxPooling2D(pool_size=(2, 2)))                                 # channel dimensions = 14x14x16   \n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model3.add(Convolution2D(32, 3, activation='relu'))#10                     # channel dimensions = 12x12x32    \n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.3))\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model3.add(Convolution2D(64, 3, activation='relu'))#8                      # channel dimensions = 10x10x64    \n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.2))\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model3.add(Convolution2D(64, 3, activation='relu'))#6                      # channel dimensions = 8x8x64   \n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.3))\n",
        "\n",
        "model3.add(Convolution2D(32, 3, activation='relu'))#6                      # channel dimensions = 6x6x16   \n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.2))\n",
        "\n",
        "model3.add(Convolution2D(32, 3, activation='relu'))#6                      # channel dimensions = 4x4x16   \n",
        "model3.add(BatchNormalization())\n",
        "\n",
        "\n",
        "# Performing only 2dconvolution at the last convolution layer(no batchnormalization and dropout)\n",
        "\n",
        "# Here we are Flateening our dat i.e making it one dimensional which we will feed to the network.\n",
        "model3.add(Flatten())\n",
        "#Dense Layer\n",
        "model3.add(Dense(16,activation='relu'))\n",
        "model3.add(Dense(10,activation='relu'))   \n",
        "#Using softmax activation function at the last layer which is used for multi class classification\n",
        "model3.add(Activation('softmax'))\n",
        "model3.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 30, 30, 10)        280       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 30, 30, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 30, 30, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 28, 28, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 28, 28, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 16)        272       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 12, 12, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 12, 12, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 10, 10, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 10, 10, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 10, 10, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 64)          36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 6, 6, 32)          18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 6, 6, 32)          128       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 6, 6, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 4, 4, 32)          9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 4, 4, 32)          128       \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 16)                8208      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                170       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 99,162\n",
            "Trainable params: 98,662\n",
            "Non-trainable params: 500\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlo2U2JNq1qW",
        "outputId": "80c4e1b2-15ab-433f-a1d4-55644e1b5b00"
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "  return round(0.001 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "#\tLearningRate = LearningRate * 1/(1 + decay * epoch) here decay is 0.319 and epoch is 40.\n",
        "\n",
        "# here we are compiling our model and using 'categorical_crossentropy' as our loss function and adam as our optimizer with learning rate =0.001 and metrics is accuracy\n",
        "model3.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "\n",
        "# Here we are traing our model using the data and using batch size of 128,number of epochs are 40 and using verbose=1 for printing out all the results.\n",
        "# In the callbacks parameter we are using the LearningRateScheduler which takes two arguments scheduler function which we built earlier to reduce the learning rate in each decay and verbose =1\n",
        "model3.fit(X_train, Y_train, batch_size=100, epochs=40, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
            "500/500 [==============================] - 54s 11ms/step - loss: 2.1742 - accuracy: 0.2136 - val_loss: 3.0134 - val_accuracy: 0.1988\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0007581501.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.5621 - accuracy: 0.4267 - val_loss: 1.9294 - val_accuracy: 0.3732\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0006105006.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.3669 - accuracy: 0.5062 - val_loss: 1.3936 - val_accuracy: 0.5022\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0005109862.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 1.2605 - accuracy: 0.5508 - val_loss: 1.4771 - val_accuracy: 0.4925\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0004393673.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 1.1859 - accuracy: 0.5762 - val_loss: 1.3581 - val_accuracy: 0.5363\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0003853565.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.1540 - accuracy: 0.5909 - val_loss: 1.3712 - val_accuracy: 0.5422\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0003431709.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 1.0982 - accuracy: 0.6086 - val_loss: 1.1717 - val_accuracy: 0.5928\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0003093102.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 1.0501 - accuracy: 0.6281 - val_loss: 1.2078 - val_accuracy: 0.5901\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0002815315.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 1.0150 - accuracy: 0.6407 - val_loss: 1.0897 - val_accuracy: 0.6243\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0002583312.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.9882 - accuracy: 0.6493 - val_loss: 1.0093 - val_accuracy: 0.6508\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0002386635.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.9611 - accuracy: 0.6596 - val_loss: 1.0120 - val_accuracy: 0.6524\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0002217787.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.9324 - accuracy: 0.6684 - val_loss: 1.0185 - val_accuracy: 0.6474\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0002071251.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.9186 - accuracy: 0.6753 - val_loss: 0.9944 - val_accuracy: 0.6582\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0001942879.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.9011 - accuracy: 0.6815 - val_loss: 0.9524 - val_accuracy: 0.6722\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0001829491.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.8911 - accuracy: 0.6842 - val_loss: 0.9122 - val_accuracy: 0.6903\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0001728608.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.8740 - accuracy: 0.6944 - val_loss: 0.8760 - val_accuracy: 0.6960\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.000163827.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.8522 - accuracy: 0.7003 - val_loss: 0.9289 - val_accuracy: 0.6838\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0001556905.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.8461 - accuracy: 0.7043 - val_loss: 0.8704 - val_accuracy: 0.7019\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0001483239.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.8392 - accuracy: 0.7056 - val_loss: 0.9119 - val_accuracy: 0.6924\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.000141623.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.8254 - accuracy: 0.7121 - val_loss: 0.8804 - val_accuracy: 0.7028\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0001355014.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.8382 - accuracy: 0.7062 - val_loss: 0.8521 - val_accuracy: 0.7110\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.000129887.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.8111 - accuracy: 0.7146 - val_loss: 0.8412 - val_accuracy: 0.7135\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0001247194.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.8090 - accuracy: 0.7144 - val_loss: 0.8199 - val_accuracy: 0.7193\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0001199472.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.8014 - accuracy: 0.7199 - val_loss: 0.8103 - val_accuracy: 0.7227\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0001155268.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.7973 - accuracy: 0.7199 - val_loss: 0.8312 - val_accuracy: 0.7185\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0001114206.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.7884 - accuracy: 0.7271 - val_loss: 0.8016 - val_accuracy: 0.7242\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0001075963.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.7796 - accuracy: 0.7240 - val_loss: 0.8080 - val_accuracy: 0.7234\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0001040258.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.7797 - accuracy: 0.7282 - val_loss: 0.7888 - val_accuracy: 0.7291\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0001006847.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.7773 - accuracy: 0.7280 - val_loss: 0.7894 - val_accuracy: 0.7321\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 9.75515e-05.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.7637 - accuracy: 0.7334 - val_loss: 0.7993 - val_accuracy: 0.7243\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler reducing learning rate to 9.46074e-05.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.7701 - accuracy: 0.7287 - val_loss: 0.8191 - val_accuracy: 0.7255\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler reducing learning rate to 9.18358e-05.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.7561 - accuracy: 0.7383 - val_loss: 0.7848 - val_accuracy: 0.7354\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler reducing learning rate to 8.9222e-05.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.7716 - accuracy: 0.7268 - val_loss: 0.7717 - val_accuracy: 0.7373\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler reducing learning rate to 8.67528e-05.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.7602 - accuracy: 0.7334 - val_loss: 0.7865 - val_accuracy: 0.7348\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler reducing learning rate to 8.44167e-05.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.7605 - accuracy: 0.7310 - val_loss: 0.8185 - val_accuracy: 0.7234\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler reducing learning rate to 8.2203e-05.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.7519 - accuracy: 0.7364 - val_loss: 0.7766 - val_accuracy: 0.7364\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler reducing learning rate to 8.01025e-05.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.7514 - accuracy: 0.7367 - val_loss: 0.7872 - val_accuracy: 0.7348\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler reducing learning rate to 7.81067e-05.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.7488 - accuracy: 0.7375 - val_loss: 0.7904 - val_accuracy: 0.7366\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler reducing learning rate to 7.62079e-05.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.7410 - accuracy: 0.7384 - val_loss: 0.7776 - val_accuracy: 0.7386\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler reducing learning rate to 7.43992e-05.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.7433 - accuracy: 0.7353 - val_loss: 0.7658 - val_accuracy: 0.7413\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb8801b8110>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH_82B9brZDr",
        "outputId": "722336ee-27eb-4474-fe0e-68ec3eb3174a"
      },
      "source": [
        "score = model3.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "y_pred = model3.predict(X_test)\n",
        "index1={}\n",
        "for i, k in enumerate(y_pred[:1][0]):\n",
        "  index1.update({k:i})\n",
        "print('Prediction:',index1[max(index1)])\n",
        "print('Actual',y_test[:1][0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.7657691836357117, 0.7412999868392944]\n",
            "Prediction: 3\n",
            "Actual [3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NISrhTkmwVKX",
        "outputId": "51dd55d5-a02b-4a97-8ff7-54ce73c2312b"
      },
      "source": [
        "model4 = Sequential()\n",
        "\n",
        "model4.add(Convolution2D(10, 3, activation='relu', input_shape=(32,32,3))) # channel dimensions = 30x30x10    \n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.3))\n",
        "\n",
        "model4.add(Convolution2D(16, 3, activation='relu'))                        # channel dimensions = 28x28x16    \n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.2))\n",
        "\n",
        "model4.add(Convolution2D(32, 5, activation='relu'))                        # channel dimensions = 24x24x32    \n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.2))\n",
        "\n",
        "model4.add(Convolution2D(64, 5, activation='relu'))                        # channel dimensions = 20x20x64   \n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.2))\n",
        "\n",
        "model4.add(Convolution2D(32, 5, activation='relu'))                        # channel dimensions = 16x16x32    \n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.2))\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model4.add(Convolution2D(64, 5, activation='relu'))#10                     # channel dimensions = 12x12x64    \n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.3))\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model4.add(Convolution2D(32, 3, activation='relu'))#8                      # channel dimensions = 10x10x32    \n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.2))\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model4.add(Convolution2D(64, 3, activation='relu'))#6                      # channel dimensions = 8x8x64   \n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.3))\n",
        "\n",
        "model4.add(Convolution2D(32, 3, activation='relu'))#6                      # channel dimensions = 6x6x32   \n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.2))\n",
        "\n",
        "model4.add(Convolution2D(16, 3, activation='relu'))#6                      # channel dimensions = 4x4x16   \n",
        "model4.add(BatchNormalization())\n",
        "\n",
        "\n",
        "# Performing only 2dconvolution at the last convolution layer(no batchnormalization and dropout)\n",
        "\n",
        "# Here we are Flateening our dat i.e making it one dimensional which we will feed to the network.\n",
        "model4.add(Flatten())\n",
        "#Dense Layer\n",
        "model4.add(Dense(10,activation='relu'))   \n",
        "#Using softmax activation function at the last layer which is used for multi class classification\n",
        "model4.add(Activation('softmax'))\n",
        "model4.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 30, 30, 10)        280       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 30, 30, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 30, 30, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 28, 28, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 28, 28, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 28, 28, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 24, 24, 32)        12832     \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 20, 20, 64)        51264     \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 20, 20, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 20, 20, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 16, 16, 32)        51232     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 16, 16, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 12, 12, 64)        51264     \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 12, 12, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 10, 10, 32)        18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 10, 10, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 10, 10, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 8, 8, 64)          18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 6, 6, 32)          18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 6, 6, 32)          128       \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 6, 6, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 4, 4, 16)          4624      \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 4, 4, 16)          64        \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 232,394\n",
            "Trainable params: 231,670\n",
            "Non-trainable params: 724\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH0DFU6PylSw",
        "outputId": "8f4c9d1a-a91e-48ad-e2a6-a365297a42fb"
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "  return round(0.001 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "#\tLearningRate = LearningRate * 1/(1 + decay * epoch) here decay is 0.319 and epoch is 40.\n",
        "\n",
        "# here we are compiling our model and using 'categorical_crossentropy' as our loss function and adam as our optimizer with learning rate =0.001 and metrics is accuracy\n",
        "model4.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "\n",
        "# Here we are traing our model using the data and using batch size of 128,number of epochs are 40 and using verbose=1 for printing out all the results.\n",
        "# In the callbacks parameter we are using the LearningRateScheduler which takes two arguments scheduler function which we built earlier to reduce the learning rate in each decay and verbose =1\n",
        "model4.fit(X_train, Y_train, batch_size=100, epochs=40, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
            "500/500 [==============================] - 13s 20ms/step - loss: 2.1653 - accuracy: 0.2218 - val_loss: 2.0240 - val_accuracy: 0.3007\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0007581501.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 1.5538 - accuracy: 0.4392 - val_loss: 1.3889 - val_accuracy: 0.5152\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0006105006.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 1.3406 - accuracy: 0.5227 - val_loss: 1.3373 - val_accuracy: 0.5330\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0005109862.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 1.2153 - accuracy: 0.5675 - val_loss: 1.1509 - val_accuracy: 0.5991\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0004393673.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 1.1302 - accuracy: 0.5995 - val_loss: 1.1542 - val_accuracy: 0.5997\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0003853565.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 1.0563 - accuracy: 0.6277 - val_loss: 1.0893 - val_accuracy: 0.6261\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0003431709.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 1.0025 - accuracy: 0.6485 - val_loss: 0.9878 - val_accuracy: 0.6609\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0003093102.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.9597 - accuracy: 0.6631 - val_loss: 0.9895 - val_accuracy: 0.6589\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0002815315.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.9198 - accuracy: 0.6752 - val_loss: 0.8908 - val_accuracy: 0.6932\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0002583312.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.8946 - accuracy: 0.6880 - val_loss: 0.8973 - val_accuracy: 0.6969\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0002386635.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.8740 - accuracy: 0.6921 - val_loss: 0.8965 - val_accuracy: 0.6964\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0002217787.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.8425 - accuracy: 0.7024 - val_loss: 0.8781 - val_accuracy: 0.7074\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0002071251.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.8185 - accuracy: 0.7118 - val_loss: 0.8916 - val_accuracy: 0.7037\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0001942879.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.8094 - accuracy: 0.7173 - val_loss: 0.8315 - val_accuracy: 0.7206\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0001829491.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.7876 - accuracy: 0.7214 - val_loss: 0.8506 - val_accuracy: 0.7156\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0001728608.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.7679 - accuracy: 0.7319 - val_loss: 0.8078 - val_accuracy: 0.7308\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.000163827.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.7537 - accuracy: 0.7320 - val_loss: 0.8361 - val_accuracy: 0.7223\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0001556905.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.7390 - accuracy: 0.7391 - val_loss: 0.7848 - val_accuracy: 0.7372\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0001483239.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.7410 - accuracy: 0.7411 - val_loss: 0.7754 - val_accuracy: 0.7403\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.000141623.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.7260 - accuracy: 0.7445 - val_loss: 0.7826 - val_accuracy: 0.7342\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0001355014.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.7272 - accuracy: 0.7455 - val_loss: 0.7739 - val_accuracy: 0.7421\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.000129887.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.7050 - accuracy: 0.7512 - val_loss: 0.7738 - val_accuracy: 0.7428\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0001247194.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.7035 - accuracy: 0.7525 - val_loss: 0.7768 - val_accuracy: 0.7403\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0001199472.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.7017 - accuracy: 0.7513 - val_loss: 0.7751 - val_accuracy: 0.7413\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0001155268.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6880 - accuracy: 0.7602 - val_loss: 0.7590 - val_accuracy: 0.7479\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0001114206.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6757 - accuracy: 0.7650 - val_loss: 0.7694 - val_accuracy: 0.7432\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0001075963.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6642 - accuracy: 0.7687 - val_loss: 0.7545 - val_accuracy: 0.7487\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0001040258.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6719 - accuracy: 0.7625 - val_loss: 0.7608 - val_accuracy: 0.7452\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0001006847.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6611 - accuracy: 0.7677 - val_loss: 0.7431 - val_accuracy: 0.7503\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 9.75515e-05.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6617 - accuracy: 0.7682 - val_loss: 0.7461 - val_accuracy: 0.7511\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler reducing learning rate to 9.46074e-05.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6548 - accuracy: 0.7683 - val_loss: 0.7486 - val_accuracy: 0.7526\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler reducing learning rate to 9.18358e-05.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6511 - accuracy: 0.7738 - val_loss: 0.7371 - val_accuracy: 0.7537\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler reducing learning rate to 8.9222e-05.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6347 - accuracy: 0.7786 - val_loss: 0.7381 - val_accuracy: 0.7574\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler reducing learning rate to 8.67528e-05.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6428 - accuracy: 0.7749 - val_loss: 0.7348 - val_accuracy: 0.7539\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler reducing learning rate to 8.44167e-05.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6281 - accuracy: 0.7795 - val_loss: 0.7338 - val_accuracy: 0.7579\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler reducing learning rate to 8.2203e-05.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6279 - accuracy: 0.7794 - val_loss: 0.7426 - val_accuracy: 0.7565\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler reducing learning rate to 8.01025e-05.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6259 - accuracy: 0.7787 - val_loss: 0.7491 - val_accuracy: 0.7531\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler reducing learning rate to 7.81067e-05.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6296 - accuracy: 0.7798 - val_loss: 0.7271 - val_accuracy: 0.7605\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler reducing learning rate to 7.62079e-05.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6230 - accuracy: 0.7816 - val_loss: 0.7498 - val_accuracy: 0.7576\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler reducing learning rate to 7.43992e-05.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6179 - accuracy: 0.7826 - val_loss: 0.7332 - val_accuracy: 0.7598\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb8335317d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMsU-2DNyfmo",
        "outputId": "a33b010f-5a82-4696-fd73-588cde78355c"
      },
      "source": [
        "score = model4.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "y_pred = model4.predict(X_test)\n",
        "index1={}\n",
        "for i, k in enumerate(y_pred[1:2][0]):\n",
        "  index1.update({k:i})\n",
        "print('Prediction:',index1[max(index1)])\n",
        "print('Actual',y_test[1:2][0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.7332082390785217, 0.7598000168800354]\n",
            "Prediction: 3\n",
            "Actual [3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-p23dTRysUF",
        "outputId": "ee772774-c612-4c66-c63d-6fe87544642f"
      },
      "source": [
        "model5 = Sequential()\n",
        "\n",
        "model5.add(Convolution2D(10, 3, activation='relu', input_shape=(32,32,3))) # channel dimensions = 30x30x10    \n",
        "model5.add(BatchNormalization())\n",
        "\n",
        "\n",
        "model5.add(Convolution2D(16, 3, activation='relu'))                        # channel dimensions = 28x28x16    \n",
        "model5.add(BatchNormalization())\n",
        "\n",
        "\n",
        "model5.add(Convolution2D(16, 1, activation='relu'))                        # channel dimensions = 28x28x16    \n",
        "model5.add(MaxPooling2D(pool_size=(2, 2)))                                 # channel dimensions = 14x14x16   \n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model5.add(Convolution2D(32, 3, activation='relu'))#10                     # channel dimensions = 12x12x32    \n",
        "model5.add(BatchNormalization())\n",
        "\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model5.add(Convolution2D(64, 3, activation='relu'))#8                      # channel dimensions = 10x10x64    \n",
        "model5.add(BatchNormalization())\n",
        "\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model5.add(Convolution2D(64, 3, activation='relu'))#6                      # channel dimensions = 8x8x64   \n",
        "model5.add(BatchNormalization())\n",
        "\n",
        "\n",
        "model5.add(Convolution2D(32, 3, activation='relu'))#6                      # channel dimensions = 6x6x32   \n",
        "model5.add(BatchNormalization())\n",
        "\n",
        "\n",
        "model5.add(Convolution2D(16, 3, activation='relu'))#6                      # channel dimensions = 4x4x16   \n",
        "model5.add(BatchNormalization())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Here we are Flateening our dat i.e making it one dimensional which we will feed to the network.\n",
        "model5.add(Flatten())\n",
        "model5.add(Dense(10,activation='relu'))\n",
        "#Using softmax activation function at the last layer which is used for multi class classification\n",
        "model5.add(Activation('softmax'))\n",
        "model5.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 30, 30, 10)        280       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 30, 30, 10)        40        \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 28, 28, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 16)        272       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 12, 12, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 12, 12, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 10, 10, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 10, 10, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 64)          36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 6, 6, 32)          18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 6, 6, 32)          128       \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 4, 4, 16)          4624      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 4, 4, 16)          64        \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 88,666\n",
            "Trainable params: 88,198\n",
            "Non-trainable params: 468\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ivk3J6ykA2HF",
        "outputId": "a6c6dea0-71c0-4855-e7e6-ac7c0513830c"
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "  return round(0.001 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "#\tLearningRate = LearningRate * 1/(1 + decay * epoch) here decay is 0.319 and epoch is 40.\n",
        "\n",
        "# here we are compiling our model and using 'categorical_crossentropy' as our loss function and adam as our optimizer with learning rate =0.001 and metrics is accuracy\n",
        "model5.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "\n",
        "# Here we are traing our model using the data and using batch size of 128,number of epochs are 40 and using verbose=1 for printing out all the results.\n",
        "# In the callbacks parameter we are using the LearningRateScheduler which takes two arguments scheduler function which we built earlier to reduce the learning rate in each decay and verbose =1\n",
        "model5.fit(X_train, Y_train, batch_size=100, epochs=30, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
            "500/500 [==============================] - 53s 10ms/step - loss: 1.9444 - accuracy: 0.3070 - val_loss: 1.5754 - val_accuracy: 0.4379\n",
            "Epoch 2/30\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0007581501.\n",
            "500/500 [==============================] - 4s 8ms/step - loss: 1.2587 - accuracy: 0.5539 - val_loss: 1.2112 - val_accuracy: 0.5783\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0006105006.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 1.0491 - accuracy: 0.6303 - val_loss: 1.1028 - val_accuracy: 0.6194\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0005109862.\n",
            "500/500 [==============================] - 4s 8ms/step - loss: 0.9132 - accuracy: 0.6802 - val_loss: 0.9831 - val_accuracy: 0.6502\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0004393673.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.8226 - accuracy: 0.7115 - val_loss: 1.0443 - val_accuracy: 0.6399\n",
            "Epoch 6/30\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0003853565.\n",
            "500/500 [==============================] - 4s 8ms/step - loss: 0.7537 - accuracy: 0.7359 - val_loss: 0.9180 - val_accuracy: 0.6815\n",
            "Epoch 7/30\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0003431709.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.6783 - accuracy: 0.7644 - val_loss: 0.9046 - val_accuracy: 0.6856\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0003093102.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.6209 - accuracy: 0.7857 - val_loss: 0.9286 - val_accuracy: 0.6897\n",
            "Epoch 9/30\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0002815315.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.5657 - accuracy: 0.8042 - val_loss: 0.9560 - val_accuracy: 0.6882\n",
            "Epoch 10/30\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0002583312.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.5201 - accuracy: 0.8238 - val_loss: 0.9273 - val_accuracy: 0.6983\n",
            "Epoch 11/30\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0002386635.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.4769 - accuracy: 0.8394 - val_loss: 0.9203 - val_accuracy: 0.7019\n",
            "Epoch 12/30\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0002217787.\n",
            "500/500 [==============================] - 4s 8ms/step - loss: 0.4415 - accuracy: 0.8544 - val_loss: 0.9509 - val_accuracy: 0.6955\n",
            "Epoch 13/30\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0002071251.\n",
            "500/500 [==============================] - 4s 8ms/step - loss: 0.4054 - accuracy: 0.8665 - val_loss: 0.9700 - val_accuracy: 0.6937\n",
            "Epoch 14/30\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0001942879.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.3694 - accuracy: 0.8812 - val_loss: 0.9902 - val_accuracy: 0.6896\n",
            "Epoch 15/30\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0001829491.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.3436 - accuracy: 0.8908 - val_loss: 1.0135 - val_accuracy: 0.6964\n",
            "Epoch 16/30\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0001728608.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.3142 - accuracy: 0.9012 - val_loss: 1.0431 - val_accuracy: 0.6938\n",
            "Epoch 17/30\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.000163827.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.2902 - accuracy: 0.9112 - val_loss: 1.0599 - val_accuracy: 0.6879\n",
            "Epoch 18/30\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0001556905.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.2666 - accuracy: 0.9189 - val_loss: 1.1042 - val_accuracy: 0.6855\n",
            "Epoch 19/30\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0001483239.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.2489 - accuracy: 0.9254 - val_loss: 1.1234 - val_accuracy: 0.6866\n",
            "Epoch 20/30\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.000141623.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.2229 - accuracy: 0.9353 - val_loss: 1.1634 - val_accuracy: 0.6846\n",
            "Epoch 21/30\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0001355014.\n",
            "500/500 [==============================] - 4s 8ms/step - loss: 0.2100 - accuracy: 0.9405 - val_loss: 1.1791 - val_accuracy: 0.6808\n",
            "Epoch 22/30\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.000129887.\n",
            "500/500 [==============================] - 4s 8ms/step - loss: 0.1970 - accuracy: 0.9439 - val_loss: 1.2030 - val_accuracy: 0.6839\n",
            "Epoch 23/30\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0001247194.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.1785 - accuracy: 0.9532 - val_loss: 1.2363 - val_accuracy: 0.6800\n",
            "Epoch 24/30\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0001199472.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.1650 - accuracy: 0.9583 - val_loss: 1.2828 - val_accuracy: 0.6730\n",
            "Epoch 25/30\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0001155268.\n",
            "500/500 [==============================] - 4s 8ms/step - loss: 0.1543 - accuracy: 0.9601 - val_loss: 1.2822 - val_accuracy: 0.6821\n",
            "Epoch 26/30\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0001114206.\n",
            "500/500 [==============================] - 4s 8ms/step - loss: 0.1422 - accuracy: 0.9657 - val_loss: 1.3159 - val_accuracy: 0.6786\n",
            "Epoch 27/30\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0001075963.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.1324 - accuracy: 0.9676 - val_loss: 1.3313 - val_accuracy: 0.6824\n",
            "Epoch 28/30\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0001040258.\n",
            "500/500 [==============================] - 4s 8ms/step - loss: 0.1216 - accuracy: 0.9716 - val_loss: 1.3606 - val_accuracy: 0.6784\n",
            "Epoch 29/30\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0001006847.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.1099 - accuracy: 0.9765 - val_loss: 1.3995 - val_accuracy: 0.6760\n",
            "Epoch 30/30\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 9.75515e-05.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.1105 - accuracy: 0.9741 - val_loss: 1.4195 - val_accuracy: 0.6731\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f468006c450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y460ghpNA8Ou",
        "outputId": "460115ff-bb2d-44c5-d027-b0ff36fdbbab"
      },
      "source": [
        "score = model5.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "y_pred = model5.predict(X_test)\n",
        "index1={}\n",
        "for i, k in enumerate(y_pred[1:2][0]):\n",
        "  index1.update({k:i})\n",
        "print('Prediction:',index1[max(index1)])\n",
        "print('Actual',y_test[1:2][0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.4194657802581787, 0.6730999946594238]\n",
            "Prediction: 8\n",
            "Actual [8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5r6uAcwBGO6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}