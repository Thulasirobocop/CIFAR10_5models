{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10_5_models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5dlD7Mq1nYB"
      },
      "source": [
        "!pip install -q keras\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add,Activation,BatchNormalization, Convolution2D, MaxPooling2D,BatchNormalization\n",
        "from keras.utils import np_utils \n",
        "from keras.datasets import mnist\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.datasets import cifar10\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k55fr1ot3G4w"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKdGeBKv3ffZ",
        "outputId": "34888df0-ce03-4f63-a533-484f373840a0"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "(50000, 1)\n",
            "(10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "GsPB7g653p4Q",
        "outputId": "b63f7496-84a7-4f40-acfa-63917fbe9d39"
      },
      "source": [
        "plt.imshow(X_train[1])\n",
        "print(y_train[1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKI_zRZm5FaJ"
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 32, 32,3)\n",
        "X_test = X_test.reshape(X_test.shape[0], 32,32,3)\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYh80dX153EU",
        "outputId": "722b112e-a960-4bdd-e909-92dd97420cc6"
      },
      "source": [
        "model1 = Sequential()\n",
        "\n",
        "model1.add(Convolution2D(10, 3, activation='relu', input_shape=(32,32,3))) # channel dimensions = 30x30x10    \n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.2))\n",
        "\n",
        "model1.add(Convolution2D(16, 3, activation='relu'))                        # channel dimensions = 28x28x16    \n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.2))\n",
        "\n",
        "model1.add(Convolution2D(16, 1, activation='relu'))                        # channel dimensions = 28x28x16    \n",
        "model1.add(MaxPooling2D(pool_size=(2, 2)))                                 # channel dimensions = 14x14x16   \n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model1.add(Convolution2D(32, 3, activation='relu'))#10                     # channel dimensions = 12x12x32    \n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.2))\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model1.add(Convolution2D(32, 3, activation='relu'))#8                      # channel dimensions = 10x10x32    \n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.2))\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model1.add(Convolution2D(32, 3, activation='relu'))#6                      # channel dimensions = 8x8x32   \n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.2))\n",
        "\n",
        "model1.add(Convolution2D(16, 3, activation='relu'))#6                      # channel dimensions = 6x6x16   \n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.2))\n",
        "\n",
        "model1.add(Convolution2D(16, 3, activation='relu'))#6                      # channel dimensions = 4x4x16   \n",
        "model1.add(BatchNormalization())\n",
        "model1.add(Dropout(0.2))\n",
        "\n",
        "# Performing only 2dconvolution at the last convolution layer(no batchnormalization and dropout)\n",
        "model1.add(Convolution2D(10, 4))                                           # using 4x4 kernel to see the complete image\n",
        "\n",
        "# Here we are Flateening our dat i.e making it one dimensional which we will feed to the network.\n",
        "model1.add(Flatten())\n",
        "#Using softmax activation function at the last layer which is used for multi class classification\n",
        "model1.add(Activation('softmax'))\n",
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 30, 30, 10)        280       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 30, 30, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 30, 30, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 28, 28, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 28, 28, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 28, 28, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 28, 28, 16)        272       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 12, 12, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 12, 12, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 10, 10, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 10, 10, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 10, 10, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 8, 8, 32)          9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 8, 8, 32)          128       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 6, 6, 16)          4624      \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 6, 6, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 4, 4, 16)          2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 4, 4, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 1, 1, 10)          2570      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 35,274\n",
            "Trainable params: 34,966\n",
            "Non-trainable params: 308\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUKE0ms-78QC",
        "outputId": "86aec673-75b2-41d6-b3a2-5c3421a74542"
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "#\tLearningRate = LearningRate * 1/(1 + decay * epoch) here decay is 0.319 and epoch is 10.\n",
        "\n",
        "# here we are compiling our model and using 'categorical_crossentropy' as our loss function and adam as our optimizer with learning rate =0.003 and metrics is accuracy\n",
        "model1.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "# Here we are traing our model using the data and using batch size of 128,number of epochs are 20 and using verbose=1 for printing out all the results.\n",
        "# In the callbacks parameter we are using the LearningRateScheduler which takes two arguments scheduler function which we built earlier to reduce the learning rate in each decay and verbose =1\n",
        "model1.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.003.\n",
            "391/391 [==============================] - 9s 14ms/step - loss: 2.0945 - accuracy: 0.2530 - val_loss: 2.3066 - val_accuracy: 0.2554\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0022744503.\n",
            "391/391 [==============================] - 5s 12ms/step - loss: 1.4272 - accuracy: 0.4756 - val_loss: 1.4288 - val_accuracy: 0.4913\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0018315018.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 1.2603 - accuracy: 0.5420 - val_loss: 1.2758 - val_accuracy: 0.5492\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0015329586.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 1.1706 - accuracy: 0.5764 - val_loss: 1.2384 - val_accuracy: 0.5607\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0013181019.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 1.0953 - accuracy: 0.6070 - val_loss: 1.0906 - val_accuracy: 0.6126\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0011560694.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 1.0492 - accuracy: 0.6250 - val_loss: 1.1537 - val_accuracy: 0.5899\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0010295127.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 1.0148 - accuracy: 0.6362 - val_loss: 1.1349 - val_accuracy: 0.6031\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0009279307.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.9660 - accuracy: 0.6552 - val_loss: 1.0511 - val_accuracy: 0.6247\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0008445946.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.9555 - accuracy: 0.6583 - val_loss: 0.9372 - val_accuracy: 0.6729\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0007749935.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.9304 - accuracy: 0.6681 - val_loss: 0.9575 - val_accuracy: 0.6642\n",
            "Epoch 11/20\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0007159905.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.9134 - accuracy: 0.6745 - val_loss: 0.9831 - val_accuracy: 0.6554\n",
            "Epoch 12/20\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.000665336.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.9022 - accuracy: 0.6799 - val_loss: 0.9531 - val_accuracy: 0.6710\n",
            "Epoch 13/20\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0006213753.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.8961 - accuracy: 0.6818 - val_loss: 0.9217 - val_accuracy: 0.6820\n",
            "Epoch 14/20\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0005828638.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.8769 - accuracy: 0.6887 - val_loss: 1.0142 - val_accuracy: 0.6542\n",
            "Epoch 15/20\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0005488474.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.8603 - accuracy: 0.6949 - val_loss: 0.8942 - val_accuracy: 0.6891\n",
            "Epoch 16/20\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0005185825.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.8560 - accuracy: 0.6943 - val_loss: 0.9268 - val_accuracy: 0.6736\n",
            "Epoch 17/20\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.000491481.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.8505 - accuracy: 0.6963 - val_loss: 0.9088 - val_accuracy: 0.6849\n",
            "Epoch 18/20\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0004670715.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.8504 - accuracy: 0.6973 - val_loss: 0.8907 - val_accuracy: 0.6873\n",
            "Epoch 19/20\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0004449718.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.8216 - accuracy: 0.7078 - val_loss: 0.9389 - val_accuracy: 0.6786\n",
            "Epoch 20/20\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.000424869.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.8343 - accuracy: 0.7038 - val_loss: 0.9648 - val_accuracy: 0.6743\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f462db2a990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFyq7Sjv-VT9",
        "outputId": "85cb95c6-b6ce-4e47-ae8d-987bbec1cd89"
      },
      "source": [
        "score = model1.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "y_pred = model1.predict(X_test)\n",
        "index1={}\n",
        "for i, k in enumerate(y_pred[:1][0]):\n",
        "  index1.update({k:i})\n",
        "print('Prediction:',index1[max(index1)])\n",
        "print('Actual',y_test[:1][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.9648078083992004, 0.6743000149726868]\n",
            "Prediction: 3\n",
            "Actual [3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izUpDSBEEBZU"
      },
      "source": [
        "model1.save('CIFAR_model1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH7DfYci8wld",
        "outputId": "bee7be91-f502-492b-8d83-fcbb006c0845"
      },
      "source": [
        "model2 = Sequential()\n",
        "\n",
        "model2.add(Convolution2D(10, 3, activation='relu', input_shape=(32,32,3))) # channel dimensions = 30x30x10    \n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.2))\n",
        "\n",
        "model2.add(Convolution2D(16, 3, activation='relu'))                        # channel dimensions = 28x28x16    \n",
        "model2.add(BatchNormalization())\n",
        "#model2.add(Dropout(0.2))\n",
        "\n",
        "model2.add(Convolution2D(16, 1, activation='relu'))                        # channel dimensions = 28x28x16    \n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))                                 # channel dimensions = 14x14x16   \n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model2.add(Convolution2D(32, 3, activation='relu'))#10                     # channel dimensions = 12x12x32    \n",
        "model2.add(BatchNormalization())\n",
        "#model2.add(Dropout(0.2))\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model2.add(Convolution2D(64, 3, activation='relu'))#8                      # channel dimensions = 10x10x64    \n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.2))\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model2.add(Convolution2D(64, 3, activation='relu'))#6                      # channel dimensions = 8x8x64   \n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.2))\n",
        "\n",
        "model2.add(Convolution2D(32, 3, activation='relu'))#6                      # channel dimensions = 6x6x32   \n",
        "model2.add(BatchNormalization())\n",
        "#model2.add(Dropout(0.2))\n",
        "\n",
        "model2.add(Convolution2D(16, 3, activation='relu'))#6                      # channel dimensions = 4x4x16   \n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(0.2))\n",
        "\n",
        "# Performing only 2dconvolution at the last convolution layer(no batchnormalization and dropout)\n",
        "model2.add(Convolution2D(10, 4))                                           # using 4x4 kernel to see the complete image\n",
        "\n",
        "# Here we are Flateening our dat i.e making it one dimensional which we will feed to the network.\n",
        "model2.add(Flatten())\n",
        "#Using softmax activation function at the last layer which is used for multi class classification\n",
        "model2.add(Activation('softmax'))\n",
        "model2.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 30, 30, 10)        280       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 30, 30, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 30, 30, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 28, 28, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 16)        272       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 12, 12, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 12, 12, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 10, 10, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 10, 10, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 10, 10, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 64)          36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 6, 6, 32)          18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 6, 6, 32)          128       \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 4, 4, 16)          4624      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 4, 4, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 1, 1, 10)          2570      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 88,666\n",
            "Trainable params: 88,198\n",
            "Non-trainable params: 468\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQp2IsmP_Swh",
        "outputId": "d10ff7ae-1f09-40c6-8d9c-1b777794eaaf"
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "  return round(0.002 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "#\tLearningRate = LearningRate * 1/(1 + decay * epoch) here decay is 0.319 and epoch is 10.\n",
        "\n",
        "# here we are compiling our model and using 'categorical_crossentropy' as our loss function and adam as our optimizer with learning rate =0.003 and metrics is accuracy\n",
        "model2.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.002), metrics=['accuracy'])\n",
        "\n",
        "# Here we are traing our model using the data and using batch size of 128,number of epochs are 20 and using verbose=1 for printing out all the results.\n",
        "# In the callbacks parameter we are using the LearningRateScheduler which takes two arguments scheduler function which we built earlier to reduce the learning rate in each decay and verbose =1\n",
        "model2.fit(X_train, Y_train, batch_size=128, epochs=25, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.002.\n",
            "391/391 [==============================] - 53s 13ms/step - loss: 1.9437 - accuracy: 0.3033 - val_loss: 2.2133 - val_accuracy: 0.2894\n",
            "Epoch 2/25\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0015163002.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 1.2613 - accuracy: 0.5453 - val_loss: 1.1758 - val_accuracy: 0.5790\n",
            "Epoch 3/25\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0012210012.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 1.0543 - accuracy: 0.6224 - val_loss: 1.0336 - val_accuracy: 0.6357\n",
            "Epoch 4/25\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0010219724.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.9439 - accuracy: 0.6651 - val_loss: 0.9492 - val_accuracy: 0.6653\n",
            "Epoch 5/25\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0008787346.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.8687 - accuracy: 0.6926 - val_loss: 0.9285 - val_accuracy: 0.6669\n",
            "Epoch 6/25\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0007707129.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.7979 - accuracy: 0.7182 - val_loss: 0.8484 - val_accuracy: 0.7026\n",
            "Epoch 7/25\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0006863418.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.7505 - accuracy: 0.7315 - val_loss: 0.8712 - val_accuracy: 0.6941\n",
            "Epoch 8/25\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0006186205.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.7234 - accuracy: 0.7440 - val_loss: 0.7917 - val_accuracy: 0.7298\n",
            "Epoch 9/25\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0005630631.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.6921 - accuracy: 0.7548 - val_loss: 0.7853 - val_accuracy: 0.7342\n",
            "Epoch 10/25\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0005166624.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.6668 - accuracy: 0.7644 - val_loss: 0.7576 - val_accuracy: 0.7371\n",
            "Epoch 11/25\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.000477327.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.6414 - accuracy: 0.7730 - val_loss: 0.7530 - val_accuracy: 0.7372\n",
            "Epoch 12/25\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0004435573.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.6292 - accuracy: 0.7781 - val_loss: 0.7451 - val_accuracy: 0.7494\n",
            "Epoch 13/25\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0004142502.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.6095 - accuracy: 0.7851 - val_loss: 0.7290 - val_accuracy: 0.7521\n",
            "Epoch 14/25\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0003885759.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.5941 - accuracy: 0.7882 - val_loss: 0.7100 - val_accuracy: 0.7617\n",
            "Epoch 15/25\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0003658983.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.5786 - accuracy: 0.7968 - val_loss: 0.6989 - val_accuracy: 0.7596\n",
            "Epoch 16/25\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0003457217.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.5814 - accuracy: 0.7933 - val_loss: 0.6800 - val_accuracy: 0.7676\n",
            "Epoch 17/25\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.000327654.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.5577 - accuracy: 0.8051 - val_loss: 0.6870 - val_accuracy: 0.7714\n",
            "Epoch 18/25\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.000311381.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.5514 - accuracy: 0.8092 - val_loss: 0.6990 - val_accuracy: 0.7670\n",
            "Epoch 19/25\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0002966479.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.5413 - accuracy: 0.8092 - val_loss: 0.7002 - val_accuracy: 0.7664\n",
            "Epoch 20/25\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.000283246.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.5334 - accuracy: 0.8092 - val_loss: 0.7046 - val_accuracy: 0.7670\n",
            "Epoch 21/25\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0002710027.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.5261 - accuracy: 0.8134 - val_loss: 0.6727 - val_accuracy: 0.7722\n",
            "Epoch 22/25\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.000259774.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.5128 - accuracy: 0.8193 - val_loss: 0.6556 - val_accuracy: 0.7814\n",
            "Epoch 23/25\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0002494388.\n",
            "391/391 [==============================] - 4s 10ms/step - loss: 0.5095 - accuracy: 0.8206 - val_loss: 0.7046 - val_accuracy: 0.7704\n",
            "Epoch 24/25\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0002398944.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.5079 - accuracy: 0.8185 - val_loss: 0.6716 - val_accuracy: 0.7786\n",
            "Epoch 25/25\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0002310536.\n",
            "391/391 [==============================] - 4s 11ms/step - loss: 0.5045 - accuracy: 0.8234 - val_loss: 0.6998 - val_accuracy: 0.7753\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc1e0171190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FR83kHD1-euk",
        "outputId": "c67c52f5-1f4d-4362-e37d-4f7e982e6a81"
      },
      "source": [
        "score = model2.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "y_pred = model2.predict(X_test)\n",
        "index1={}\n",
        "for i, k in enumerate(y_pred[2:3][0]):\n",
        "  index1.update({k:i})\n",
        "print('Prediction:',index1[max(index1)])\n",
        "print('Actual',y_test[2:3][0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6997845768928528, 0.7753000259399414]\n",
            "Prediction: 8\n",
            "Actual [8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDndQstrkc_9"
      },
      "source": [
        "model2.save('CIFAR_model2.h5')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n1iLy9q_ewc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e3e178-87c7-4ddb-ffab-5d8b7f8bcbc3"
      },
      "source": [
        "model3 = Sequential()\n",
        "\n",
        "model3.add(Convolution2D(10, 3, activation='relu', input_shape=(32,32,3))) # channel dimensions = 30x30x10    \n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.3))\n",
        "\n",
        "model3.add(Convolution2D(16, 3, activation='relu'))                        # channel dimensions = 28x28x16    \n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.2))\n",
        "\n",
        "model3.add(Convolution2D(16, 1, activation='relu'))                        # channel dimensions = 28x28x16    \n",
        "model3.add(MaxPooling2D(pool_size=(2, 2)))                                 # channel dimensions = 14x14x16   \n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model3.add(Convolution2D(32, 3, activation='relu'))#10                     # channel dimensions = 12x12x32    \n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.3))\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model3.add(Convolution2D(64, 3, activation='relu'))#8                      # channel dimensions = 10x10x64    \n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.2))\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model3.add(Convolution2D(64, 3, activation='relu'))#6                      # channel dimensions = 8x8x64   \n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.3))\n",
        "\n",
        "model3.add(Convolution2D(32, 3, activation='relu'))#6                      # channel dimensions = 6x6x16   \n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.2))\n",
        "\n",
        "model3.add(Convolution2D(32, 3, activation='relu'))#6                      # channel dimensions = 4x4x16   \n",
        "model3.add(BatchNormalization())\n",
        "\n",
        "\n",
        "# Performing only 2dconvolution at the last convolution layer(no batchnormalization and dropout)\n",
        "\n",
        "# Here we are Flateening our dat i.e making it one dimensional which we will feed to the network.\n",
        "model3.add(Flatten())\n",
        "#Dense Layer\n",
        "model3.add(Dense(16,activation='relu'))\n",
        "model3.add(Dense(10,activation='relu'))   \n",
        "#Using softmax activation function at the last layer which is used for multi class classification\n",
        "model3.add(Activation('softmax'))\n",
        "model3.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 30, 30, 10)        280       \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 30, 30, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 30, 30, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 28, 28, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 28, 28, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 28, 28, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 28, 28, 16)        272       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 12, 12, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 12, 12, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 12, 12, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 10, 10, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 10, 10, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 10, 10, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 8, 8, 64)          36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 6, 6, 32)          18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 6, 6, 32)          128       \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 6, 6, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 4, 4, 32)          9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 4, 4, 32)          128       \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 16)                8208      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                170       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 99,162\n",
            "Trainable params: 98,662\n",
            "Non-trainable params: 500\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlo2U2JNq1qW",
        "outputId": "9c2b9233-a889-4448-afab-8c5d1dfd26c6"
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "  return round(0.001 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "#\tLearningRate = LearningRate * 1/(1 + decay * epoch) here decay is 0.319 and epoch is 40.\n",
        "\n",
        "# here we are compiling our model and using 'categorical_crossentropy' as our loss function and adam as our optimizer with learning rate =0.001 and metrics is accuracy\n",
        "model3.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "\n",
        "# Here we are traing our model using the data and using batch size of 128,number of epochs are 40 and using verbose=1 for printing out all the results.\n",
        "# In the callbacks parameter we are using the LearningRateScheduler which takes two arguments scheduler function which we built earlier to reduce the learning rate in each decay and verbose =1\n",
        "model3.fit(X_train, Y_train, batch_size=100, epochs=40, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
            "500/500 [==============================] - 8s 11ms/step - loss: 2.1229 - accuracy: 0.2180 - val_loss: 2.0626 - val_accuracy: 0.2510\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0007581501.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 1.5570 - accuracy: 0.4305 - val_loss: 1.6355 - val_accuracy: 0.4127\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0006105006.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 1.3603 - accuracy: 0.5118 - val_loss: 1.4488 - val_accuracy: 0.4873\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0005109862.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 1.2425 - accuracy: 0.5557 - val_loss: 1.3414 - val_accuracy: 0.5265\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0004393673.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 1.1695 - accuracy: 0.5837 - val_loss: 1.3155 - val_accuracy: 0.5350\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0003853565.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 1.1149 - accuracy: 0.6040 - val_loss: 1.2087 - val_accuracy: 0.5797\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0003431709.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 1.0630 - accuracy: 0.6233 - val_loss: 1.1691 - val_accuracy: 0.5880\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0003093102.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 1.0296 - accuracy: 0.6375 - val_loss: 1.1779 - val_accuracy: 0.5923\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0002815315.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.9938 - accuracy: 0.6514 - val_loss: 1.1238 - val_accuracy: 0.6077\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0002583312.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.9846 - accuracy: 0.6513 - val_loss: 1.1031 - val_accuracy: 0.6178\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0002386635.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.9491 - accuracy: 0.6662 - val_loss: 1.0639 - val_accuracy: 0.6324\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0002217787.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.9428 - accuracy: 0.6704 - val_loss: 1.0604 - val_accuracy: 0.6351\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0002071251.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.9348 - accuracy: 0.6733 - val_loss: 1.0391 - val_accuracy: 0.6471\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0001942879.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.9212 - accuracy: 0.6777 - val_loss: 0.9894 - val_accuracy: 0.6602\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0001829491.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.8951 - accuracy: 0.6858 - val_loss: 1.0427 - val_accuracy: 0.6494\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0001728608.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.8986 - accuracy: 0.6855 - val_loss: 0.9734 - val_accuracy: 0.6684\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.000163827.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.8782 - accuracy: 0.6908 - val_loss: 0.9425 - val_accuracy: 0.6776\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0001556905.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.8666 - accuracy: 0.6956 - val_loss: 0.9629 - val_accuracy: 0.6668\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0001483239.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.8675 - accuracy: 0.6945 - val_loss: 0.9291 - val_accuracy: 0.6840\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.000141623.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.8533 - accuracy: 0.6970 - val_loss: 0.9883 - val_accuracy: 0.6628\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0001355014.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.8504 - accuracy: 0.7014 - val_loss: 0.9656 - val_accuracy: 0.6738\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.000129887.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.8407 - accuracy: 0.7068 - val_loss: 0.9029 - val_accuracy: 0.6912\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0001247194.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.8335 - accuracy: 0.7067 - val_loss: 0.9671 - val_accuracy: 0.6699\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0001199472.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.8275 - accuracy: 0.7113 - val_loss: 0.9618 - val_accuracy: 0.6745\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0001155268.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.8171 - accuracy: 0.7139 - val_loss: 0.8980 - val_accuracy: 0.6948\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0001114206.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.8333 - accuracy: 0.7058 - val_loss: 0.9137 - val_accuracy: 0.6894\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0001075963.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.8170 - accuracy: 0.7133 - val_loss: 0.9450 - val_accuracy: 0.6753\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0001040258.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.8174 - accuracy: 0.7139 - val_loss: 0.8722 - val_accuracy: 0.6993\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0001006847.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.8121 - accuracy: 0.7144 - val_loss: 0.9140 - val_accuracy: 0.6883\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 9.75515e-05.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.8057 - accuracy: 0.7154 - val_loss: 0.8985 - val_accuracy: 0.6915\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler reducing learning rate to 9.46074e-05.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.7925 - accuracy: 0.7208 - val_loss: 0.8895 - val_accuracy: 0.6956\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler reducing learning rate to 9.18358e-05.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.7937 - accuracy: 0.7204 - val_loss: 0.8587 - val_accuracy: 0.7075\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler reducing learning rate to 8.9222e-05.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.7946 - accuracy: 0.7214 - val_loss: 0.8873 - val_accuracy: 0.6961\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler reducing learning rate to 8.67528e-05.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.7880 - accuracy: 0.7263 - val_loss: 0.8670 - val_accuracy: 0.7033\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler reducing learning rate to 8.44167e-05.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.7858 - accuracy: 0.7230 - val_loss: 0.8773 - val_accuracy: 0.7015\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler reducing learning rate to 8.2203e-05.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.7789 - accuracy: 0.7270 - val_loss: 0.8497 - val_accuracy: 0.7091\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler reducing learning rate to 8.01025e-05.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.7765 - accuracy: 0.7272 - val_loss: 0.8666 - val_accuracy: 0.7052\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler reducing learning rate to 7.81067e-05.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.7668 - accuracy: 0.7329 - val_loss: 0.8629 - val_accuracy: 0.7050\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler reducing learning rate to 7.62079e-05.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.7621 - accuracy: 0.7323 - val_loss: 0.8634 - val_accuracy: 0.7061\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler reducing learning rate to 7.43992e-05.\n",
            "500/500 [==============================] - 5s 10ms/step - loss: 0.7638 - accuracy: 0.7329 - val_loss: 0.8777 - val_accuracy: 0.7009\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc19856e490>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH_82B9brZDr",
        "outputId": "30985bb2-2f57-4253-c8f1-d54ea939868e"
      },
      "source": [
        "score = model3.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "y_pred = model3.predict(X_test)\n",
        "index1={}\n",
        "for i, k in enumerate(y_pred[:1][0]):\n",
        "  index1.update({k:i})\n",
        "print('Prediction:',index1[max(index1)])\n",
        "print('Actual',y_test[:1][0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.8777137398719788, 0.7009000182151794]\n",
            "Prediction: 3\n",
            "Actual [3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D-xj1jglKGk"
      },
      "source": [
        "model3.save('CIFAR_model3.h5')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NISrhTkmwVKX",
        "outputId": "2aacd29f-ce35-4f8c-ffdf-aa487676c9d4"
      },
      "source": [
        "model4 = Sequential()\n",
        "\n",
        "model4.add(Convolution2D(10, 3, activation='relu', input_shape=(32,32,3))) # channel dimensions = 30x30x10    \n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.3))\n",
        "\n",
        "model4.add(Convolution2D(16, 3, activation='relu'))                        # channel dimensions = 28x28x16    \n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.2))\n",
        "\n",
        "model4.add(Convolution2D(32, 5, activation='relu'))                        # channel dimensions = 24x24x32    \n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.2))\n",
        "\n",
        "model4.add(Convolution2D(64, 5, activation='relu'))                        # channel dimensions = 20x20x64   \n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.2))\n",
        "\n",
        "model4.add(Convolution2D(32, 5, activation='relu'))                        # channel dimensions = 16x16x32    \n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.2))\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model4.add(Convolution2D(64, 5, activation='relu'))#10                     # channel dimensions = 12x12x64    \n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.3))\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model4.add(Convolution2D(32, 3, activation='relu'))#8                      # channel dimensions = 10x10x32    \n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.2))\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model4.add(Convolution2D(64, 3, activation='relu'))#6                      # channel dimensions = 8x8x64   \n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.3))\n",
        "\n",
        "model4.add(Convolution2D(32, 3, activation='relu'))#6                      # channel dimensions = 6x6x32   \n",
        "model4.add(BatchNormalization())\n",
        "model4.add(Dropout(0.2))\n",
        "\n",
        "model4.add(Convolution2D(16, 3, activation='relu'))#6                      # channel dimensions = 4x4x16   \n",
        "model4.add(BatchNormalization())\n",
        "\n",
        "\n",
        "# Performing only 2dconvolution at the last convolution layer(no batchnormalization and dropout)\n",
        "\n",
        "# Here we are Flateening our dat i.e making it one dimensional which we will feed to the network.\n",
        "model4.add(Flatten())\n",
        "#Dense Layer\n",
        "model4.add(Dense(10,activation='relu'))   \n",
        "#Using softmax activation function at the last layer which is used for multi class classification\n",
        "model4.add(Activation('softmax'))\n",
        "model4.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 30, 30, 10)        280       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 30, 30, 10)        40        \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 30, 30, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 28, 28, 16)        64        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 28, 28, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 32)        12832     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 20, 20, 64)        51264     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 20, 20, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 20, 20, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 32)        51232     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 16, 16, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 12, 12, 64)        51264     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 12, 12, 64)        256       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 10, 10, 32)        18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 10, 10, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 10, 10, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 8, 8, 64)          18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 6, 6, 32)          18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 6, 6, 32)          128       \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 6, 6, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 4, 4, 16)          4624      \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 4, 4, 16)          64        \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 232,394\n",
            "Trainable params: 231,670\n",
            "Non-trainable params: 724\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH0DFU6PylSw",
        "outputId": "49a0a7f9-3ef9-41b9-c7ea-03a8e96c0ded"
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "  return round(0.001 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "#\tLearningRate = LearningRate * 1/(1 + decay * epoch) here decay is 0.319 and epoch is 40.\n",
        "\n",
        "# here we are compiling our model and using 'categorical_crossentropy' as our loss function and adam as our optimizer with learning rate =0.001 and metrics is accuracy\n",
        "model4.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "\n",
        "# Here we are traing our model using the data and using batch size of 128,number of epochs are 40 and using verbose=1 for printing out all the results.\n",
        "# In the callbacks parameter we are using the LearningRateScheduler which takes two arguments scheduler function which we built earlier to reduce the learning rate in each decay and verbose =1\n",
        "model4.fit(X_train, Y_train, batch_size=100, epochs=40, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
            "500/500 [==============================] - 34s 20ms/step - loss: 2.2203 - accuracy: 0.1956 - val_loss: 2.3719 - val_accuracy: 0.2245\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0007581501.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 1.6482 - accuracy: 0.3955 - val_loss: 1.6214 - val_accuracy: 0.4393\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0006105006.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 1.3977 - accuracy: 0.4982 - val_loss: 1.2449 - val_accuracy: 0.5568\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0005109862.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 1.2475 - accuracy: 0.5557 - val_loss: 1.1921 - val_accuracy: 0.5879\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0004393673.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 1.1462 - accuracy: 0.5902 - val_loss: 1.1684 - val_accuracy: 0.5882\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0003853565.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 1.0821 - accuracy: 0.6154 - val_loss: 1.0621 - val_accuracy: 0.6375\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0003431709.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 1.0287 - accuracy: 0.6372 - val_loss: 0.9753 - val_accuracy: 0.6633\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0003093102.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.9723 - accuracy: 0.6583 - val_loss: 0.9835 - val_accuracy: 0.6671\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0002815315.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.9291 - accuracy: 0.6715 - val_loss: 0.9096 - val_accuracy: 0.6845\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0002583312.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.8953 - accuracy: 0.6864 - val_loss: 0.8809 - val_accuracy: 0.6984\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0002386635.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.8681 - accuracy: 0.6949 - val_loss: 0.8693 - val_accuracy: 0.7044\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0002217787.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.8461 - accuracy: 0.7023 - val_loss: 0.8434 - val_accuracy: 0.7128\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0002071251.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.8160 - accuracy: 0.7122 - val_loss: 0.8342 - val_accuracy: 0.7141\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0001942879.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.7978 - accuracy: 0.7182 - val_loss: 0.8113 - val_accuracy: 0.7183\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0001829491.\n",
            "500/500 [==============================] - 9s 19ms/step - loss: 0.7884 - accuracy: 0.7240 - val_loss: 0.8296 - val_accuracy: 0.7178\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0001728608.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.7604 - accuracy: 0.7341 - val_loss: 0.8008 - val_accuracy: 0.7274\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.000163827.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.7562 - accuracy: 0.7361 - val_loss: 0.7999 - val_accuracy: 0.7307\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0001556905.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.7346 - accuracy: 0.7419 - val_loss: 0.7726 - val_accuracy: 0.7391\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0001483239.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.7220 - accuracy: 0.7493 - val_loss: 0.7647 - val_accuracy: 0.7417\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.000141623.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.7070 - accuracy: 0.7520 - val_loss: 0.7678 - val_accuracy: 0.7390\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0001355014.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.7082 - accuracy: 0.7499 - val_loss: 0.7692 - val_accuracy: 0.7429\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.000129887.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6974 - accuracy: 0.7542 - val_loss: 0.7479 - val_accuracy: 0.7502\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0001247194.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6886 - accuracy: 0.7564 - val_loss: 0.7447 - val_accuracy: 0.7520\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0001199472.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6823 - accuracy: 0.7602 - val_loss: 0.7465 - val_accuracy: 0.7517\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0001155268.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6646 - accuracy: 0.7641 - val_loss: 0.7354 - val_accuracy: 0.7561\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0001114206.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6674 - accuracy: 0.7645 - val_loss: 0.7365 - val_accuracy: 0.7514\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0001075963.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6596 - accuracy: 0.7693 - val_loss: 0.7342 - val_accuracy: 0.7556\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0001040258.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6524 - accuracy: 0.7726 - val_loss: 0.7335 - val_accuracy: 0.7546\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0001006847.\n",
            "500/500 [==============================] - 9s 19ms/step - loss: 0.6517 - accuracy: 0.7735 - val_loss: 0.7285 - val_accuracy: 0.7583\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 9.75515e-05.\n",
            "500/500 [==============================] - 9s 19ms/step - loss: 0.6448 - accuracy: 0.7737 - val_loss: 0.7289 - val_accuracy: 0.7604\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler reducing learning rate to 9.46074e-05.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6303 - accuracy: 0.7779 - val_loss: 0.7294 - val_accuracy: 0.7566\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler reducing learning rate to 9.18358e-05.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6215 - accuracy: 0.7809 - val_loss: 0.7439 - val_accuracy: 0.7585\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler reducing learning rate to 8.9222e-05.\n",
            "500/500 [==============================] - 9s 19ms/step - loss: 0.6324 - accuracy: 0.7759 - val_loss: 0.7278 - val_accuracy: 0.7599\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler reducing learning rate to 8.67528e-05.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6226 - accuracy: 0.7816 - val_loss: 0.7167 - val_accuracy: 0.7651\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler reducing learning rate to 8.44167e-05.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6197 - accuracy: 0.7828 - val_loss: 0.7179 - val_accuracy: 0.7632\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler reducing learning rate to 8.2203e-05.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6145 - accuracy: 0.7840 - val_loss: 0.7133 - val_accuracy: 0.7653\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler reducing learning rate to 8.01025e-05.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6102 - accuracy: 0.7851 - val_loss: 0.7025 - val_accuracy: 0.7643\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler reducing learning rate to 7.81067e-05.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6052 - accuracy: 0.7895 - val_loss: 0.7174 - val_accuracy: 0.7631\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler reducing learning rate to 7.62079e-05.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6064 - accuracy: 0.7870 - val_loss: 0.7018 - val_accuracy: 0.7686\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler reducing learning rate to 7.43992e-05.\n",
            "500/500 [==============================] - 9s 18ms/step - loss: 0.6009 - accuracy: 0.7891 - val_loss: 0.7024 - val_accuracy: 0.7649\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa44a4361d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMsU-2DNyfmo",
        "outputId": "a87fd43f-36d5-4f77-a732-c9c68ca46f5b"
      },
      "source": [
        "score = model4.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "y_pred = model4.predict(X_test)\n",
        "index1={}\n",
        "for i, k in enumerate(y_pred[1:2][0]):\n",
        "  index1.update({k:i})\n",
        "print('Prediction:',index1[max(index1)])\n",
        "print('Actual',y_test[1:2][0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.7024291157722473, 0.7649999856948853]\n",
            "Prediction: 1\n",
            "Actual [8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSC2OP-JmK77"
      },
      "source": [
        "model4.save('CIFAR_model4.h5')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-p23dTRysUF",
        "outputId": "ee772774-c612-4c66-c63d-6fe87544642f"
      },
      "source": [
        "model5 = Sequential()\n",
        "\n",
        "model5.add(Convolution2D(10, 3, activation='relu', input_shape=(32,32,3))) # channel dimensions = 30x30x10    \n",
        "model5.add(BatchNormalization())\n",
        "\n",
        "\n",
        "model5.add(Convolution2D(16, 3, activation='relu'))                        # channel dimensions = 28x28x16    \n",
        "model5.add(BatchNormalization())\n",
        "\n",
        "\n",
        "model5.add(Convolution2D(16, 1, activation='relu'))                        # channel dimensions = 28x28x16    \n",
        "model5.add(MaxPooling2D(pool_size=(2, 2)))                                 # channel dimensions = 14x14x16   \n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model5.add(Convolution2D(32, 3, activation='relu'))#10                     # channel dimensions = 12x12x32    \n",
        "model5.add(BatchNormalization())\n",
        "\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model5.add(Convolution2D(64, 3, activation='relu'))#8                      # channel dimensions = 10x10x64    \n",
        "model5.add(BatchNormalization())\n",
        "\n",
        "\n",
        "# Performing 2dconvolution followed by BatchNormalization and Dropout\n",
        "model5.add(Convolution2D(64, 3, activation='relu'))#6                      # channel dimensions = 8x8x64   \n",
        "model5.add(BatchNormalization())\n",
        "\n",
        "\n",
        "model5.add(Convolution2D(32, 3, activation='relu'))#6                      # channel dimensions = 6x6x32   \n",
        "model5.add(BatchNormalization())\n",
        "\n",
        "\n",
        "model5.add(Convolution2D(16, 3, activation='relu'))#6                      # channel dimensions = 4x4x16   \n",
        "model5.add(BatchNormalization())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Here we are Flateening our dat i.e making it one dimensional which we will feed to the network.\n",
        "model5.add(Flatten())\n",
        "model5.add(Dense(10,activation='relu'))\n",
        "#Using softmax activation function at the last layer which is used for multi class classification\n",
        "model5.add(Activation('softmax'))\n",
        "model5.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 30, 30, 10)        280       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 30, 30, 10)        40        \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 16)        1456      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 28, 28, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 16)        272       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 12, 12, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 12, 12, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 10, 10, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 10, 10, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 64)          36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 6, 6, 32)          18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 6, 6, 32)          128       \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 4, 4, 16)          4624      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 4, 4, 16)          64        \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 88,666\n",
            "Trainable params: 88,198\n",
            "Non-trainable params: 468\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ivk3J6ykA2HF",
        "outputId": "a6c6dea0-71c0-4855-e7e6-ac7c0513830c"
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "  return round(0.001 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "#\tLearningRate = LearningRate * 1/(1 + decay * epoch) here decay is 0.319 and epoch is 40.\n",
        "\n",
        "# here we are compiling our model and using 'categorical_crossentropy' as our loss function and adam as our optimizer with learning rate =0.001 and metrics is accuracy\n",
        "model5.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "\n",
        "# Here we are traing our model using the data and using batch size of 128,number of epochs are 40 and using verbose=1 for printing out all the results.\n",
        "# In the callbacks parameter we are using the LearningRateScheduler which takes two arguments scheduler function which we built earlier to reduce the learning rate in each decay and verbose =1\n",
        "model5.fit(X_train, Y_train, batch_size=100, epochs=30, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
            "500/500 [==============================] - 53s 10ms/step - loss: 1.9444 - accuracy: 0.3070 - val_loss: 1.5754 - val_accuracy: 0.4379\n",
            "Epoch 2/30\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0007581501.\n",
            "500/500 [==============================] - 4s 8ms/step - loss: 1.2587 - accuracy: 0.5539 - val_loss: 1.2112 - val_accuracy: 0.5783\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0006105006.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 1.0491 - accuracy: 0.6303 - val_loss: 1.1028 - val_accuracy: 0.6194\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0005109862.\n",
            "500/500 [==============================] - 4s 8ms/step - loss: 0.9132 - accuracy: 0.6802 - val_loss: 0.9831 - val_accuracy: 0.6502\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0004393673.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.8226 - accuracy: 0.7115 - val_loss: 1.0443 - val_accuracy: 0.6399\n",
            "Epoch 6/30\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0003853565.\n",
            "500/500 [==============================] - 4s 8ms/step - loss: 0.7537 - accuracy: 0.7359 - val_loss: 0.9180 - val_accuracy: 0.6815\n",
            "Epoch 7/30\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0003431709.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.6783 - accuracy: 0.7644 - val_loss: 0.9046 - val_accuracy: 0.6856\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0003093102.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.6209 - accuracy: 0.7857 - val_loss: 0.9286 - val_accuracy: 0.6897\n",
            "Epoch 9/30\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0002815315.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.5657 - accuracy: 0.8042 - val_loss: 0.9560 - val_accuracy: 0.6882\n",
            "Epoch 10/30\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0002583312.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.5201 - accuracy: 0.8238 - val_loss: 0.9273 - val_accuracy: 0.6983\n",
            "Epoch 11/30\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0002386635.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.4769 - accuracy: 0.8394 - val_loss: 0.9203 - val_accuracy: 0.7019\n",
            "Epoch 12/30\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0002217787.\n",
            "500/500 [==============================] - 4s 8ms/step - loss: 0.4415 - accuracy: 0.8544 - val_loss: 0.9509 - val_accuracy: 0.6955\n",
            "Epoch 13/30\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0002071251.\n",
            "500/500 [==============================] - 4s 8ms/step - loss: 0.4054 - accuracy: 0.8665 - val_loss: 0.9700 - val_accuracy: 0.6937\n",
            "Epoch 14/30\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0001942879.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.3694 - accuracy: 0.8812 - val_loss: 0.9902 - val_accuracy: 0.6896\n",
            "Epoch 15/30\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0001829491.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.3436 - accuracy: 0.8908 - val_loss: 1.0135 - val_accuracy: 0.6964\n",
            "Epoch 16/30\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0001728608.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.3142 - accuracy: 0.9012 - val_loss: 1.0431 - val_accuracy: 0.6938\n",
            "Epoch 17/30\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.000163827.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.2902 - accuracy: 0.9112 - val_loss: 1.0599 - val_accuracy: 0.6879\n",
            "Epoch 18/30\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0001556905.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.2666 - accuracy: 0.9189 - val_loss: 1.1042 - val_accuracy: 0.6855\n",
            "Epoch 19/30\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0001483239.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.2489 - accuracy: 0.9254 - val_loss: 1.1234 - val_accuracy: 0.6866\n",
            "Epoch 20/30\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.000141623.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.2229 - accuracy: 0.9353 - val_loss: 1.1634 - val_accuracy: 0.6846\n",
            "Epoch 21/30\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0001355014.\n",
            "500/500 [==============================] - 4s 8ms/step - loss: 0.2100 - accuracy: 0.9405 - val_loss: 1.1791 - val_accuracy: 0.6808\n",
            "Epoch 22/30\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.000129887.\n",
            "500/500 [==============================] - 4s 8ms/step - loss: 0.1970 - accuracy: 0.9439 - val_loss: 1.2030 - val_accuracy: 0.6839\n",
            "Epoch 23/30\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0001247194.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.1785 - accuracy: 0.9532 - val_loss: 1.2363 - val_accuracy: 0.6800\n",
            "Epoch 24/30\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0001199472.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.1650 - accuracy: 0.9583 - val_loss: 1.2828 - val_accuracy: 0.6730\n",
            "Epoch 25/30\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0001155268.\n",
            "500/500 [==============================] - 4s 8ms/step - loss: 0.1543 - accuracy: 0.9601 - val_loss: 1.2822 - val_accuracy: 0.6821\n",
            "Epoch 26/30\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0001114206.\n",
            "500/500 [==============================] - 4s 8ms/step - loss: 0.1422 - accuracy: 0.9657 - val_loss: 1.3159 - val_accuracy: 0.6786\n",
            "Epoch 27/30\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0001075963.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.1324 - accuracy: 0.9676 - val_loss: 1.3313 - val_accuracy: 0.6824\n",
            "Epoch 28/30\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0001040258.\n",
            "500/500 [==============================] - 4s 8ms/step - loss: 0.1216 - accuracy: 0.9716 - val_loss: 1.3606 - val_accuracy: 0.6784\n",
            "Epoch 29/30\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0001006847.\n",
            "500/500 [==============================] - 4s 9ms/step - loss: 0.1099 - accuracy: 0.9765 - val_loss: 1.3995 - val_accuracy: 0.6760\n",
            "Epoch 30/30\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 9.75515e-05.\n",
            "500/500 [==============================] - 5s 9ms/step - loss: 0.1105 - accuracy: 0.9741 - val_loss: 1.4195 - val_accuracy: 0.6731\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f468006c450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y460ghpNA8Ou",
        "outputId": "460115ff-bb2d-44c5-d027-b0ff36fdbbab"
      },
      "source": [
        "score = model5.evaluate(X_test, Y_test, verbose=0)\n",
        "print(score)\n",
        "y_pred = model5.predict(X_test)\n",
        "index1={}\n",
        "for i, k in enumerate(y_pred[1:2][0]):\n",
        "  index1.update({k:i})\n",
        "print('Prediction:',index1[max(index1)])\n",
        "print('Actual',y_test[1:2][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.4194657802581787, 0.6730999946594238]\n",
            "Prediction: 8\n",
            "Actual [8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5r6uAcwBGO6"
      },
      "source": [
        "model5.save('CIFAR_model5.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q1jK3XLD4FC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}